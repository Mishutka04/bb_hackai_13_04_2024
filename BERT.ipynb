{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzX79D_f9lr3"
      },
      "source": [
        "# Устанавливаем зависимости"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O9JJeDEmYP_q",
        "outputId": "c13b1f63-f318-4e8e-ac22-d8761d32e59d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting textract\n",
            "  Downloading textract-1.6.5-py3-none-any.whl (23 kB)\n",
            "Collecting argcomplete~=1.10.0 (from textract)\n",
            "  Downloading argcomplete-1.10.3-py2.py3-none-any.whl (36 kB)\n",
            "Collecting beautifulsoup4~=4.8.0 (from textract)\n",
            "  Downloading beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet==3.* (from textract)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docx2txt~=0.8 (from textract)\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting extract-msg<=0.29.* (from textract)\n",
            "  Downloading extract_msg-0.28.7-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20191110 (from textract)\n",
            "  Downloading pdfminer.six-20191110-py2.py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-pptx~=0.6.18 (from textract)\n",
            "  Downloading python_pptx-0.6.23-py3-none-any.whl (471 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting six~=1.12.0 (from textract)\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting SpeechRecognition~=3.8.1 (from textract)\n",
            "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xlrd~=1.2.0 (from textract)\n",
            "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodome (from pdfminer.six==20191110->textract)\n",
            "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
            "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4~=4.8.0->textract) (2.5)\n",
            "Collecting imapclient==2.1.0 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting olefile>=0.46 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tzlocal>=2.1 in /usr/local/lib/python3.10/dist-packages (from extract-msg<=0.29.*->textract) (5.2)\n",
            "Collecting compressed-rtf>=1.0.6 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ebcdic>=1.1.1 (from extract-msg<=0.29.*->textract)\n",
            "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.5/128.5 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx~=0.6.18->textract) (4.9.4)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx~=0.6.18->textract) (9.4.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx~=0.6.18->textract)\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docx2txt, compressed-rtf\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3960 sha256=a985d5b24b4010e2289245201ecaa1b9c58fbcecf82b148ca5591cfdd84ae3fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6185 sha256=fee810ddd131e145eb6b5a45cbc89d7b125c526c53b7696084e264f60978ab8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/3e/48/e7d833ecc516c36f8966d310b1a6386db091a718f1ff3bf85c\n",
            "Successfully built docx2txt compressed-rtf\n",
            "Installing collected packages: SpeechRecognition, ebcdic, docx2txt, compressed-rtf, chardet, argcomplete, XlsxWriter, xlrd, six, pycryptodome, olefile, beautifulsoup4, python-pptx, pdfminer.six, imapclient, extract-msg, textract\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 2.0.1\n",
            "    Uninstalling xlrd-2.0.1:\n",
            "      Successfully uninstalled xlrd-2.0.1\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.12.3\n",
            "    Uninstalling beautifulsoup4-4.12.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.12.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydrive2 1.6.3 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "yfinance 0.2.37 requires beautifulsoup4>=4.11.1, but you have beautifulsoup4 4.8.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed SpeechRecognition-3.8.1 XlsxWriter-3.2.0 argcomplete-1.10.3 beautifulsoup4-4.8.2 chardet-3.0.4 compressed-rtf-1.0.6 docx2txt-0.8 ebcdic-1.1.1 extract-msg-0.28.7 imapclient-2.1.0 olefile-0.47 pdfminer.six-20191110 pycryptodome-3.20.0 python-pptx-0.6.23 six-1.12.0 textract-1.6.5 xlrd-1.2.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "195dceec2a234d839d982411eae58e9f",
              "pip_warning": {
                "packages": [
                  "chardet",
                  "six"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.10/dist-packages (0.8)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.11.0)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  antiword\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 118 kB of archives.\n",
            "After this operation, 603 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 antiword amd64 0.37-16 [118 kB]\n",
            "Fetched 118 kB in 1s (95.9 kB/s)\n",
            "Selecting previously unselected package antiword.\n",
            "(Reading database ... 121752 files and directories currently installed.)\n",
            "Preparing to unpack .../antiword_0.37-16_amd64.deb ...\n",
            "Unpacking antiword (0.37-16) ...\n",
            "Setting up antiword (0.37-16) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install textract\n",
        "!pip install docx2txt\n",
        "!pip install python-docx\n",
        "!apt-get install antiword"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9zK5m1V9n4O"
      },
      "source": [
        "# Подготовка Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYU1Bt0yYZUp",
        "outputId": "64eae272-2c4c-4a35-bc80-8554a997852a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "СОГЛАШЕНИЕ N 8\n",
            "о расторжении трудового договора от 18.10.2010 N 15/к\n",
            "\n",
            "г. Казань                                                                                                            15 октября 2016 г.\n",
            "\n",
            "Общество с ограниченной ответственностью \"СтройМир\", именуемое в дальнейшем \"Работодатель\", в лице генерального директора Зуева Валерия Павловича, действующего на основании устава, с одной стороны, и менеджер по продажам ООО «СтройМир» Ивова Надежда Артемьевна, именуемая в дальнейшем \"Работник\", с другой стороны, вместе именуемые \"Стороны\", заключили настоящее соглашение о нижеследующем. \n",
            "1. Прекратить трудовой договор от 18.10.2010 N 15/к, заключенный между Работником и Работодателем, согласно п. 1 ст. 77 ТК РФ. \n",
            "2. Последним рабочим днем Работника считать 16 ноября 2016 г. \n",
            "3. Работодатель обязуется: \n",
            "3.1. Перед увольнением предоставить Работнику оплачиваемый отпуск на 28 календарных дней с 20 октября по 16 ноября 2016 г. \n",
            "3.2. 19 октября 2016 г. выплатить Работнику выходное пособие в размере одного месячного оклада. \n",
            "3.3. 19 октября 2016 г. возвратить Работнику трудовую книжку с соответствующей записью и полностью с ним рассчитаться. \n",
            "4. Работник обязуется передать все дела менеджеру отдела продаж до 20 октября 2016 г. \n",
            "5. Стороны подтверждают, что размер выходного пособия, установленный в п. 3.2 настоящего соглашения, является окончательным и не может быть изменен (дополнен). \n",
            "6. Стороны не имеют друг к другу претензий. \n",
            "7. Настоящее соглашение составлено в двух экземплярах, имеющих одинаковую юридическую силу, по одному для каждой из Сторон.\n",
            "\n",
            "Генеральный директор ООО «СтройМир»                     Зуев                           Зуев В.П.\n",
            "\n",
            "Менеджер по продажам ООО «СтройМир»                   Ивова                         Ивова Н.А.\n",
            "\n",
            "СОГЛАШЕНИЕ N 8\n",
            "о расторжении трудового договора от 18.10.2010 N 15/к\n",
            "\n",
            "г. Казань                                                                                                            15 октября 2016 г.\n",
            "\n",
            "Общество с ограниченной ответственностью \"СтройМир\", именуемое в дальнейшем \"Работодатель\", в лице генерального директора Зуева Валерия Павловича, действующего на основании устава, с одной стороны, и менеджер по продажам ООО «СтройМир» Ивова Надежда Артемьевна, именуемая в дальнейшем \"Работник\", с другой стороны, вместе именуемые \"Стороны\", заключили настоящее соглашение о нижеследующем. \n",
            "1. Прекратить трудовой договор от 18.10.2010 N 15/к, заключенный между Работником и Работодателем, согласно п. 1 ст. 77 ТК РФ. \n",
            "2. Последним рабочим днем Работника считать 16 ноября 2016 г. \n",
            "3. Работодатель обязуется: \n",
            "3.1. Перед увольнением предоставить Работнику оплачиваемый отпуск на 28 календарных дней с 20 октября по 16 ноября 2016 г. \n",
            "3.2. 19 октября 2016 г. выплатить Работнику выходное пособие в размере одного месячного оклада. \n",
            "3.3. 19 октября 2016 г. возвратить Работнику трудовую книжку с соответствующей записью и полностью с ним рассчитаться. \n",
            "4. Работник обязуется передать все дела менеджеру отдела продаж до 20 октября 2016 г. \n",
            "5. Стороны подтверждают, что размер выходного пособия, установленный в п. 3.2 настоящего соглашения, является окончательным и не может быть изменен (дополнен). \n",
            "6. Стороны не имеют друг к другу претензий. \n",
            "7. Настоящее соглашение составлено в двух экземплярах, имеющих одинаковую юридическую силу, по одному для каждой из Сторон.\n",
            "\n",
            "Генеральный директор ООО «СтройМир»                     Зуев                           Зуев В.П.\n",
            "\n",
            "Менеджер по продажам ООО «СтройМир»                   Ивова                         Ивова Н.А.\n",
            "\n",
            "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import docx2txt  # Для чтения .doc файлов\n",
        "import textract\n",
        "import re\n",
        "import docx\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    # Удаление всех символов, кроме букв и цифр\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z0-9а-яА-Я\\s]', '', text)\n",
        "    # Удаление лишних пробелов\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
        "    cleaned_text = cleaned_text.lower()\n",
        "    return cleaned_text\n",
        "\n",
        "# Функция для чтения текста из файлов .docx\n",
        "def read_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    text = \"\"\n",
        "    for paragraph in doc.paragraphs:\n",
        "        text += paragraph.text\n",
        "    text = clean_text(text)\n",
        "    return text\n",
        "\n",
        "# Функция для чтения текста из файлов .doc\n",
        "def read_doc(file_path):\n",
        "    text = textract.process(file_path)\n",
        "    text = text.decode(\"utf-8\")\n",
        "    text = clean_text(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "labels_dict = {\n",
        "    \"invoice\": 0,\n",
        "    \"application\": 1,\n",
        "    \"statute\": 2,\n",
        "    \"arrangement\": 3,\n",
        "    \"determination\": 4,\n",
        "    \"act\": 5,\n",
        "    \"contract offer\": 6,\n",
        "    \"order\": 7,\n",
        "    \"bill\": 8,\n",
        "    \"proxy\": 9,\n",
        "    \"contract\":10,\n",
        "}\n",
        "\n",
        "csv_dataset = pd.read_csv(\"sample.csv\")\n",
        "csv_dataset=csv_dataset.drop (index= 490)\n",
        "# Инициализация списков для хранения текстов и меток классов\n",
        "documents = csv_dataset['text'].values.tolist()\n",
        "print(documents[0])\n",
        "class_list = csv_dataset['class'].values.tolist()\n",
        "labels = []\n",
        "for i in class_list:\n",
        "  labels.append(labels_dict[i])\n",
        "\n",
        "\n",
        "# Печать количества загруженных документов и уникальных меток классов\n",
        "print(documents[0])\n",
        "print(labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsU56zNa9udY"
      },
      "source": [
        "# Обучаем модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtLtaFU7qkrv",
        "outputId": "d648180b-a876-4d1f-ef0f-337e02ef442b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "450 450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Average Loss: 2.0854981230423513\n",
            "Epoch 2, Average Loss: 1.2615662219118229\n",
            "Epoch 3, Average Loss: 0.6492455560812908\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Предварительно обученная модель BERT и токенизатор\n",
        "model_name = 'bert-base-multilingual-cased'  # Используем предварительно обученную модель BERT\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=11)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "#type_list = [documents.index(doc) for doc in documents if type(doc) == float]\n",
        "X_train, X_test, y_train, y_test = train_test_split(documents, labels,\n",
        "                                                    test_size = 0.1,\n",
        "                                                    random_state = 42)\n",
        "# Токенизация текстов и преобразование в тензоры\n",
        "tokenized_texts = [tokenizer.encode(doc, add_special_tokens=True, max_length=512, truncation=True) for doc in X_train]\n",
        "max_len = max(len(tokenized_text) for tokenized_text in tokenized_texts)\n",
        "input_ids = torch.tensor([tokenized_text + [0] * (max_len - len(tokenized_text)) for tokenized_text in tokenized_texts])\n",
        "\n",
        "# Создание тензора меток\n",
        "labels = torch.tensor(y_train)\n",
        "\n",
        "print(len(input_ids), len(labels))\n",
        "# Создание DataLoader для эффективной обработки данных в пакетах\n",
        "dataset = TensorDataset(input_ids, labels)\n",
        "batch_size = 4  # Размер пакета\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Обучение модели\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        batch_input_ids, batch_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_input_ids.to(device), labels=batch_labels.to(device))\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f'Epoch {epoch + 1}, Average Loss: {avg_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO1N35OL9zPW"
      },
      "source": [
        "# Предсказываем данные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2CQH4YpzSwk",
        "outputId": "85d67a84-924c-4327-bb22-a1e033678871"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Создаем словарь для сопоставления числовых меток с их текстовыми представлениями\n",
        "labels_dict = {\n",
        "    \"invoice\": 0,\n",
        "    \"application\": 1,\n",
        "    \"statute\": 2,\n",
        "    \"arrangement\": 3,\n",
        "    \"determination\": 4,\n",
        "    \"act\": 5,\n",
        "    \"contract offer\": 6,\n",
        "    \"order\": 7,\n",
        "    \"bill\": 8,\n",
        "    \"proxy\": 9,\n",
        "    \"contract\":10,\n",
        "}\n",
        "labels_mapping = {\n",
        "    0: \"invoice\",\n",
        "    1: \"application\",\n",
        "    2: \"statute\",\n",
        "    3: \"arrangement\",\n",
        "    4: \"determination\",\n",
        "    5: \"act\",\n",
        "    6: \"contract offer\",\n",
        "    7:\"order\",\n",
        "    8:\"bill\",\n",
        "    9:\"proxy\",\n",
        "    10:\"contract\",\n",
        "\n",
        "}\n",
        "# Предсказание категорий для новых документов\n",
        "print(X_test)\n",
        "test = [doc.lower() for doc in X_test[:10]]\n",
        "\n",
        "tokenized_new_texts = [tokenizer.encode(doc.lower(), add_special_tokens=True, max_length=512, truncation=True) for doc in test]\n",
        "\n",
        "# Дополнение последовательностей токенов до максимальной длины\n",
        "padded_input_ids_new = pad_sequence([torch.tensor(tokenized_text) for tokenized_text in tokenized_new_texts], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "# Предсказание категорий\n",
        "model.to('cpu')\n",
        "predictions = model(padded_input_ids_new.to('cpu'))\n",
        "predicted_labels = torch.argmax(predictions.logits, dim=1)\n",
        "\n",
        "# Преобразуем числовные метки в их текстовые представления с помощью словаря\n",
        "predicted_labels_text = [label.item() for label in predicted_labels]\n",
        "print(\"Predicted Labels:\", predicted_labels_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGcGGO3q93OK"
      },
      "source": [
        "# Проверяем метрику"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPthokonnyDa",
        "outputId": "6bb72b91-c838-4b8a-d3d4-17271c0285b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 score per class: [0. 1. 1. 0. 0. 1. 1.]\n",
            "Micro-average F1 score: 0.8000000000000002\n",
            "Macro-average F1 score: 0.5714285714285714\n",
            "Weighted-average F1 score: 0.8\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "# Calculate F1 score for each class separately\n",
        "f1_per_class = f1_score(predicted_labels_text, y_test[:10], average=None)\n",
        "\n",
        "# Calculate micro-average F1 score\n",
        "f1_micro = f1_score(predicted_labels_text, y_test[:10], average='micro')\n",
        "\n",
        "# Calculate macro-average F1 score\n",
        "f1_macro = f1_score(predicted_labels_text, y_test[:10], average='macro')\n",
        "\n",
        "# Calculate weighted-average F1 score\n",
        "f1_weighted = f1_score(predicted_labels_text, y_test[:10], average='weighted')\n",
        "\n",
        "print(\"F1 score per class:\", f1_per_class)\n",
        "print(\"Micro-average F1 score:\", f1_micro)\n",
        "print(\"Macro-average F1 score:\", f1_macro)\n",
        "print(\"Weighted-average F1 score:\", f1_weighted)\n",
        "print(accuracy_score(predicted_labels_text, y_test[:10]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
